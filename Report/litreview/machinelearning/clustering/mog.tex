\subsubsection{Mixtures of Gaussians}
 \label{subsection:mog}

Mixtures of Gaussians (MoG), or the Gaussian Mixture Model (GMM), is a method of clustering (see section \ref{subsection:clustering}) that is based not on distance \ref{subsubsection:kmeans} but distribution. This method of clustering was popularized by Duda and Hart in their text \emph{Pattern Classification and Scene Analysis} \cite{mog_seminal}. This method is more effective than distance based methods because it considers the covariance of the data. Note that for K-Means clustering the distributions were spherical, but if the true clusters were more elliptical then K-Means would fail to cluster them properly. Figure \ref{fig:cluster_shapes} compares what these cluster shapes might look like and the arrows indicate the location, the arrows indicate data points that are ambiguous.

\begin{figure}[h]
	\centering
	\includegraphics[width = 0.9\textwidth]{litreview/computervision/mog/mog_vs_kmeans.png}
	\captionsetup{format = hang}
	\caption{Comparison of K-Means cluster shape to GMM cluster shape. Img: Hongning Wang, University of Virginia }
	\label{fig:cluster_shapes}
\end{figure}
	
The basis of the Gaussian Mixture Model is of course the Gaussian Distribution function \ref{section:gaussian}. It is useful for probabilistic clustering because, as the Central Limit Theorem States, the distribution of the average value of the subsets of any data set will approximately converge to a Normal distribution as the number of terms in the subsets increases \cite{patterns_machine_learning}. The Mixture of Gaussian for the random variable $\boldsymbol{x}$ is achieved by super-positioning the distributions of random variable for each cluster. Equation \ref{eq:mog} describes the super-positioning of $K$ distributions for random variable $\boldsymbol{x}$, the resulting curve models the distribution of the random variable. 

\begin{equation}
p(\boldsymbol{x}) = \sum^{K}_{k = 1}\pi_k \mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k}, \boldsymbol{\Sigma_k})
\label{eq:mog}
\end{equation}

The variable $\pi_k$ is known as the \emph{mixing coefficient} of a Gaussian distribution which controls the weighting of a particular distribution in the mixture. The sum of the mixing ratios is 1 because the complete mixture distribution represent the probability of a sample belonging to any one cluster and it must belong to one, hence

\[\sum_{k=1}^K\pi_k = 1\]

To help to understand how equation \ref{eq:mog} came about consider the random variable $\boldsymbol{z}$ of dimension K which is a binary random variable, meaning it can only have value 1 or 0. Only one element of $\boldsymbol{z}$ can be equal to 1 and all others must be 0, thus

\[\sum_{k=1}^Kz_k = 1\]

Hence there are K possible combinations of the vector $\boldsymbol{z}$. The marginal distribution over $\boldsymbol{z}$ is defined in terms of the mixing coefficients, 

\[p(z_k = 1) = \pi_k\]

That is to say the probability of $z_k$ = 1 is equal to the mixing coefficient of Gaussian distribution k. This can also be written

\[p(\boldsymbol{z}) = \prod^K_{k=1}\pi^{z_k}_k\]

The significance of $\boldsymbol{z}$ represents the assignment of a sample to a cluster which is why only one of its elements can be 1. To observe the Gaussian distribution of $\boldsymbol{z}$ in cluster $K$ we set $z_k = 1$ as described by the condition distribution,

\[p(\bm{x}|\boldsymbol{z}) = \mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k}, \boldsymbol{\Sigma_k}) \]

Finally we can define the marginal distribution of $\boldsymbol{x}$, formerly described by \ref{eq:mog}, using the marginal distribution $p(\boldsymbol{z})$ and the conditional distribution $p(\bm{x}|\bm{z})$ as described by equation \ref{eq:mog_derive}. Figure \ref{fig:mog_compare} shows the super-positioning of individual Gaussian distributions to form a mixture of Gaussian for a one-dimensional dataset.

\begin{align}
	p(\bm{x}) 	&= p(\bm{z})p(\bm{x}|\bm{z})\\
				&= \sum^{K}_{k = 1}\pi_k \mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k}, \boldsymbol{\Sigma_k})
\label{eq:mog_derive}
\end{align}


\begin{figure}[htbp]
    \centering
     \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{litreview/machinelearning/clustering/mog/individual_gauss.png}
	\captionsetup{format = hang}
        \caption{Three individual cluster distributions.}
        \label{fig:mog_singles}
    \end{subfigure} 
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{litreview/machinelearning/clustering/mog/combined_mog.png}	
	\captionsetup{format = hang}
        \caption{Super-positioned Gaussian distributions}
        \label{fig:mog_combined}
    \end{subfigure}
    \captionsetup{format = hang}
    \caption{Visualization of a Mixture of Gaussians for 1D dataset.}
    \label{fig:mog_compare}
\end{figure}

\input{litreview/machinelearning/clustering/em}



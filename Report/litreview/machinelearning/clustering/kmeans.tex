\subsubsection{K-Means}
\label{subsubsection:kmeans}
K-Means is simple and reasonably fast clustering algorithm. K represented the number of cluster the algorithm should create and means refers to the average value of each cluster. Given a set of data their similarity is measured as the value of the Euclidean distance between them. The general form of the Euclidean distance formula is described in equation \ref{eq:euclid}, $p$ and $q$ represent n-dimensional feature vectors. As the K-means algorithm executes, data samples are assigned to the cluster whose average value is closest to its own, the centroids are updated after each round of assignments until their values converge. Initially the average value of all clusters is randomly selected, cluster average values are referred to as cluster centroids. The algorithm to implement the K-means method is outlined in algorithm \ref{algorithm:kmeans}.

\begin{equation}
    EuclideanDistance(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 +\hdots + (p_n - q_n)^2}
    \label{eq:euclid}
\end{equation} 

\begin{algorithm}
    \SetAlgoLined
    \KwInput{Set of data vectors X of size M} 
    \KwOutput{K sets of clustered data vectors}
    Initialize the desired number of clusters $K$\;
    Initialise a list of $K$ random cluster centroids $\mu$\;
    \While{$\mu$ elements have not converged}{
        \For{i = 0 to M}
        {
            distOld = $\inf$
            \For{j = 0 to K}{
                distnew = EuclideanDistance(X[i], $\mu[j]$)\;
                \If{distNew $<$ distOld}{
                    distOld = distNew\;
                    $\mu_j$.append(X[i])\;
                }
            }
        }
        \For{p = 0 to K}{
            centroidList.append(average($\mu[p]$))\;
        }
    }
    \caption{K Means Clustering \cite{oreilly_python}}
    \label{algorithm:kmeans}
\end{algorithm}

The best result for this method is defined as having the smallest intracluster variance. K-Means is disadvantaged by the implicit trait that it formulates clusters of similar sizes. This happens because the algorithm seeks to minimize variance (spread) in each cluster hence the \q{ideal} centroid placement will form distributions spherically about centroids. Figure \ref{fig:clusters} visualizes an example of clustering data where it can be observed that the cluster sizes are similar and spherical in shape.  This method clustering does not consider any probabilistic model in classifying data, this type of classification is called a \emph{hard assignment} because a data sample \emph{definitely} belongs to one cluster. The dual of this is a \emph{soft assignment} which considers the probability of a sample's class assignment where a sample will have a probability of belonging to any cluster. 

\begin{figure}[H]
    \centering
    \centering\includegraphics[width=0.9\textwidth]{kmeans_clusters}
    \caption{K-Means clustering performed on random data. \cite{matlab_clustering}}
    \label{fig:clusters}
\end{figure} 